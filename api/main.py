"""
FastAPI Application for Vercel Deployment
Generated by vercel-fastapi-link skill v1.3.0

Project: Physical AI Book API
GitHub Pages: https://faiqahm.github.io
"""

import os
import logging
from datetime import datetime
from typing import Optional, List

from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

# RAG Dependencies
try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import Filter, FieldCondition, MatchValue
    from openai import OpenAI
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False

# =============================================================================
# Logging Configuration
# =============================================================================
# Configure logging so agents can read Vercel logs for debugging
# Vercel captures stdout/stderr, so we use StreamHandler

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("Physical AI Book API".lower().replace(" ", "-"))

# Adjust log level from environment (DEBUG, INFO, WARNING, ERROR)
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logger.setLevel(getattr(logging, log_level, logging.INFO))

# =============================================================================
# Pydantic Models - Define your request/response schemas here
# =============================================================================

class HealthResponse(BaseModel):
    """Health check response model."""
    status: str = Field(..., example="healthy")
    service: str = Field(..., example="physical-ai-book-api")
    timestamp: str = Field(..., example="2024-01-01T12:00:00Z")


class ChapterSummary(BaseModel):
    """Summary of a chapter for listing."""
    id: int = Field(..., example=1)
    title: str = Field(..., example="Introduction to Physical AI")
    slug: str = Field(..., example="intro")


class ChapterDetail(BaseModel):
    """Full chapter details."""
    id: int
    title: str
    content: str = Field(..., example="Chapter content goes here...")
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


class ChapterListResponse(BaseModel):
    """Response model for chapter listing."""
    chapters: List[ChapterSummary]
    total: int = Field(..., example=3)


class ErrorResponse(BaseModel):
    """Standard error response model."""
    detail: str = Field(..., example="Resource not found")
    error_code: Optional[str] = Field(None, example="NOT_FOUND")


# =============================================================================
# Personalization Models
# =============================================================================

class UserPreferences(BaseModel):
    """User learning preferences."""
    preferred_language: str = Field(default="en", example="en")
    difficulty_level: str = Field(default="beginner", example="intermediate")
    topics_of_interest: List[str] = Field(default=[], example=["robotics", "ai"])
    learning_style: str = Field(default="visual", example="hands-on")
    session_duration_minutes: int = Field(default=30, example=45)


class UserProfile(BaseModel):
    """User profile for personalization."""
    user_id: str = Field(..., example="user-123")
    email: Optional[str] = Field(None, example="user@example.com")
    display_name: Optional[str] = Field(None, example="John Doe")
    preferences: UserPreferences = Field(default_factory=UserPreferences)
    created_at: str = Field(default="", example="2024-01-01T12:00:00Z")
    updated_at: str = Field(default="", example="2024-01-01T12:00:00Z")


class Recommendation(BaseModel):
    """Content recommendation."""
    id: str = Field(..., example="rec-001")
    title: str = Field(..., example="Introduction to ROS 2")
    description: str = Field(..., example="Learn the basics of ROS 2")
    chapter_id: int = Field(..., example=1)
    relevance_score: float = Field(..., ge=0, le=1, example=0.95)
    reason: str = Field(..., example="Based on your interest in robotics")


class RecommendationsResponse(BaseModel):
    """Response with personalized recommendations."""
    user_id: str
    recommendations: List[Recommendation]
    generated_at: str


class LearningPathItem(BaseModel):
    """Single item in learning path."""
    order: int = Field(..., example=1)
    chapter_id: int = Field(..., example=1)
    title: str = Field(..., example="Getting Started")
    status: str = Field(default="not_started", example="in_progress")
    progress_percent: int = Field(default=0, ge=0, le=100, example=50)
    estimated_duration_minutes: int = Field(default=30, example=45)


class LearningPath(BaseModel):
    """User's personalized learning path."""
    user_id: str
    path_id: str = Field(..., example="path-001")
    title: str = Field(default="My Learning Journey", example="Physical AI Mastery")
    items: List[LearningPathItem]
    overall_progress_percent: int = Field(default=0, ge=0, le=100)
    created_at: str
    updated_at: str


class PersonalizationSettings(BaseModel):
    """Settings to apply for personalization."""
    user_id: str = Field(..., example="user-123")
    preferences: UserPreferences
    notifications_enabled: bool = Field(default=True)
    theme: str = Field(default="auto", example="dark")


class PersonalizationApplyResponse(BaseModel):
    """Response after applying personalization settings."""
    success: bool
    message: str
    applied_at: str


# =============================================================================
# RAG Chatbot Models
# =============================================================================

class ChatMessage(BaseModel):
    """A single chat message."""
    role: str = Field(..., example="user")
    content: str = Field(..., example="What is ROS 2?")


class ChatRequest(BaseModel):
    """Request for chat endpoint."""
    message: str = Field(..., min_length=1, max_length=500, example="What is Physical AI?")
    language: Optional[str] = Field(default="en", example="en")
    chapter: Optional[int] = Field(default=None, example=1)
    history: Optional[List[ChatMessage]] = Field(default=[], example=[])


class Citation(BaseModel):
    """Citation reference to source material."""
    chapter: Optional[int] = Field(None, example=1)
    section: str = Field(..., example="Introduction to ROS 2")
    source: str = Field(..., example="docs/chapter-1/index.md")


class ChatResponse(BaseModel):
    """Response from chat endpoint."""
    answer: str = Field(..., example="Physical AI refers to...")
    citations: List[Citation] = Field(default=[])
    confidence: float = Field(..., ge=0, le=1, example=0.92)
    response_time_ms: int = Field(..., example=850)
    cached: bool = Field(default=False)


# =============================================================================
# App Configuration
# =============================================================================

app = FastAPI(
    title="Physical AI Book API",
    description="Backend API for the Physical AI Educational Book",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

logger.info("FastAPI application initialized")

# =============================================================================
# CORS Configuration - Allow GitHub Pages frontend
# =============================================================================

# Get GitHub Pages URL from environment or use default
GITHUB_PAGES_URL = os.getenv("GITHUB_PAGES_URL", "https://faiqahm.github.io")

# Base allowed origins
allowed_origins = [
    "http://localhost:3000",      # Local Docusaurus dev
    "http://localhost:8000",      # Local FastAPI dev
    "http://127.0.0.1:3000",
    "http://127.0.0.1:8000",
    GITHUB_PAGES_URL,             # Production GitHub Pages
    f"{GITHUB_PAGES_URL}/",       # With trailing slash
]

# Add extra origins from environment variable (comma-separated)
extra_origins_env = os.getenv("EXTRA_CORS_ORIGINS", "")
if extra_origins_env:
    for origin in extra_origins_env.split(","):
        origin = origin.strip()
        if origin and origin not in allowed_origins:
            allowed_origins.append(origin)

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
    allow_headers=["Authorization", "Content-Type", "Accept", "Origin"],
    max_age=600,
)

# =============================================================================
# Health Check Endpoint
# =============================================================================

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for monitoring."""
    logger.debug("Health check requested")
    return HealthResponse(
        status="healthy",
        service="physical-ai-book-api",
        timestamp=datetime.utcnow().isoformat() + "Z",
    )


@app.get("/")
async def root():
    """Root endpoint with API info."""
    logger.info("Root endpoint accessed")
    return {
        "message": "Physical AI Book API",
        "docs": "/docs",
        "health": "/health",
    }

# =============================================================================
# API Routes - Add your endpoints below
# =============================================================================

@app.get("/api/v1/chapters", response_model=ChapterListResponse)
async def list_chapters():
    """List all available chapters."""
    logger.info("Listing all chapters")
    chapters = [
        ChapterSummary(id=1, title="Introduction to Physical AI", slug="intro"),
        ChapterSummary(id=2, title="Sensors and Perception", slug="chapter-1"),
        ChapterSummary(id=3, title="Actuators and Control", slug="chapter-2"),
    ]
    logger.debug(f"Returning {len(chapters)} chapters")
    return ChapterListResponse(chapters=chapters, total=len(chapters))


@app.get(
    "/api/v1/chapters/{chapter_id}",
    response_model=ChapterDetail,
    responses={404: {"model": ErrorResponse}},
)
async def get_chapter(chapter_id: int):
    """Get a specific chapter by ID."""
    logger.info(f"Fetching chapter with id={chapter_id}")

    chapters = {
        1: ChapterDetail(id=1, title="Introduction to Physical AI", content="..."),
        2: ChapterDetail(id=2, title="Sensors and Perception", content="..."),
        3: ChapterDetail(id=3, title="Actuators and Control", content="..."),
    }

    if chapter_id not in chapters:
        logger.warning(f"Chapter not found: id={chapter_id}")
        raise HTTPException(status_code=404, detail="Chapter not found")

    logger.debug(f"Returning chapter: {chapters[chapter_id].title}")
    return chapters[chapter_id]


# =============================================================================
# Personalization Endpoints
# =============================================================================

# In-memory storage for demo (replace with database in production)
_user_profiles: dict = {}
_user_learning_paths: dict = {}


@app.get(
    "/api/personalization/profile",
    response_model=UserProfile,
    responses={404: {"model": ErrorResponse}},
)
async def get_user_profile(user_id: str):
    """Get user profile for personalization."""
    logger.info(f"Fetching profile for user_id={user_id}")

    if user_id in _user_profiles:
        logger.debug(f"Found existing profile for {user_id}")
        return _user_profiles[user_id]

    # Return default profile for new users
    logger.info(f"Creating default profile for new user {user_id}")
    default_profile = UserProfile(
        user_id=user_id,
        preferences=UserPreferences(),
        created_at=datetime.utcnow().isoformat() + "Z",
        updated_at=datetime.utcnow().isoformat() + "Z",
    )
    _user_profiles[user_id] = default_profile
    return default_profile


@app.post("/api/personalization/profile", response_model=UserProfile)
async def update_user_profile(profile: UserProfile):
    """Create or update user profile."""
    logger.info(f"Updating profile for user_id={profile.user_id}")

    profile.updated_at = datetime.utcnow().isoformat() + "Z"
    if profile.user_id not in _user_profiles:
        profile.created_at = profile.updated_at

    _user_profiles[profile.user_id] = profile
    logger.debug(f"Profile saved for {profile.user_id}")
    return profile


@app.get("/api/personalization/recommendations", response_model=RecommendationsResponse)
async def get_recommendations(user_id: str):
    """Get AI-driven content recommendations for user."""
    logger.info(f"Generating recommendations for user_id={user_id}")

    # Get user preferences for personalization
    profile = _user_profiles.get(user_id)
    topics = profile.preferences.topics_of_interest if profile else []

    # Generate recommendations based on preferences (demo logic)
    recommendations = [
        Recommendation(
            id="rec-001",
            title="Introduction to Physical AI",
            description="Start your journey into Physical AI and robotics",
            chapter_id=1,
            relevance_score=0.95,
            reason="Recommended starting point for all learners",
        ),
        Recommendation(
            id="rec-002",
            title="ROS 2 Fundamentals",
            description="Learn the Robot Operating System",
            chapter_id=1,
            relevance_score=0.88,
            reason="Essential foundation for robotics" + (f" - matches your interest in {topics[0]}" if topics else ""),
        ),
        Recommendation(
            id="rec-003",
            title="Simulation with Gazebo",
            description="Practice in a virtual environment",
            chapter_id=2,
            relevance_score=0.82,
            reason="Hands-on learning without hardware",
        ),
    ]

    logger.debug(f"Generated {len(recommendations)} recommendations for {user_id}")
    return RecommendationsResponse(
        user_id=user_id,
        recommendations=recommendations,
        generated_at=datetime.utcnow().isoformat() + "Z",
    )


@app.get("/api/personalization/learning-path", response_model=LearningPath)
async def get_learning_path(user_id: str):
    """Get personalized learning path for user."""
    logger.info(f"Fetching learning path for user_id={user_id}")

    if user_id in _user_learning_paths:
        logger.debug(f"Found existing learning path for {user_id}")
        return _user_learning_paths[user_id]

    # Generate default learning path
    logger.info(f"Creating default learning path for {user_id}")
    now = datetime.utcnow().isoformat() + "Z"

    learning_path = LearningPath(
        user_id=user_id,
        path_id=f"path-{user_id}",
        title="Physical AI Learning Journey",
        items=[
            LearningPathItem(
                order=1,
                chapter_id=1,
                title="Introduction to Physical AI & ROS 2",
                status="not_started",
                progress_percent=0,
                estimated_duration_minutes=60,
            ),
            LearningPathItem(
                order=2,
                chapter_id=2,
                title="Simulation with Gazebo",
                status="not_started",
                progress_percent=0,
                estimated_duration_minutes=90,
            ),
            LearningPathItem(
                order=3,
                chapter_id=3,
                title="Vision-Language-Action Models",
                status="not_started",
                progress_percent=0,
                estimated_duration_minutes=120,
            ),
        ],
        overall_progress_percent=0,
        created_at=now,
        updated_at=now,
    )

    _user_learning_paths[user_id] = learning_path
    return learning_path


@app.post("/api/personalization/apply", response_model=PersonalizationApplyResponse)
async def apply_personalization(settings: PersonalizationSettings):
    """Apply personalization settings for user."""
    logger.info(f"Applying personalization for user_id={settings.user_id}")

    # Update or create profile with new preferences
    profile = _user_profiles.get(settings.user_id)
    now = datetime.utcnow().isoformat() + "Z"

    if profile:
        profile.preferences = settings.preferences
        profile.updated_at = now
    else:
        profile = UserProfile(
            user_id=settings.user_id,
            preferences=settings.preferences,
            created_at=now,
            updated_at=now,
        )

    _user_profiles[settings.user_id] = profile

    logger.info(f"Personalization applied for {settings.user_id}: theme={settings.theme}, notifications={settings.notifications_enabled}")

    return PersonalizationApplyResponse(
        success=True,
        message=f"Personalization settings applied successfully for user {settings.user_id}",
        applied_at=now,
    )


# =============================================================================
# RAG Chatbot Service
# =============================================================================

# Initialize RAG clients (lazy loading)
_qdrant_client = None
_openai_client = None
_response_cache: dict = {}  # Simple in-memory cache

QDRANT_COLLECTION = "textbook_chapters"
EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"

SYSTEM_PROMPT = """You are a helpful assistant for the Physical AI Educational Book.
Your role is to answer questions about Physical AI, ROS 2, Gazebo simulation, and Vision-Language-Action models
based ONLY on the provided context from the book.

Rules:
1. Only answer questions using the provided context from the book
2. If the context doesn't contain relevant information, say "I don't have information about that in the book content"
3. Always cite the chapter and section when providing information
4. Keep answers concise but comprehensive
5. For code questions, explain what the code does in plain language
6. Never make up or hallucinate information not in the context"""


def get_qdrant_client():
    """Get or create Qdrant client."""
    global _qdrant_client
    if _qdrant_client is None and QDRANT_AVAILABLE:
        qdrant_url = os.getenv("QDRANT_URL")
        qdrant_key = os.getenv("QDRANT_API_KEY")
        if qdrant_url and qdrant_key:
            _qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_key)
    return _qdrant_client


def get_openai_client():
    """Get or create OpenAI client."""
    global _openai_client
    if _openai_client is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            _openai_client = OpenAI(api_key=api_key)
    return _openai_client


def embed_query(text: str) -> list:
    """Generate embedding for query text."""
    client = get_openai_client()
    if not client:
        raise HTTPException(status_code=503, detail="OpenAI service unavailable")
    response = client.embeddings.create(input=text[:8000], model=EMBEDDING_MODEL)
    return response.data[0].embedding


def retrieve_context(query: str, language: str = "en", chapter: int = None, top_k: int = 5) -> list:
    """Retrieve relevant chunks from Qdrant."""
    client = get_qdrant_client()
    if not client:
        logger.warning("Qdrant unavailable, returning empty context")
        return []

    query_vector = embed_query(query)

    # Build filter
    filter_conditions = []
    if language:
        filter_conditions.append(FieldCondition(key="language", match=MatchValue(value=language)))
    if chapter is not None:
        filter_conditions.append(FieldCondition(key="chapter", match=MatchValue(value=chapter)))

    query_filter = Filter(must=filter_conditions) if filter_conditions else None

    try:
        # Try new API first (qdrant-client >= 1.7)
        try:
            results = client.query_points(
                collection_name=QDRANT_COLLECTION,
                query=query_vector,
                query_filter=query_filter,
                limit=top_k
            )
            points = results.points
        except AttributeError:
            # Fall back to old API
            results = client.search(
                collection_name=QDRANT_COLLECTION,
                query_vector=query_vector,
                query_filter=query_filter,
                limit=top_k
            )
            points = results

        logger.info(f"Qdrant returned {len(points)} results")
        return [
            {
                "content": hit.payload.get("content", ""),
                "section": hit.payload.get("section", "Unknown"),
                "chapter": hit.payload.get("chapter"),
                "source": hit.payload.get("source", ""),
                "score": hit.score
            }
            for hit in points
        ]
    except Exception as e:
        logger.error(f"Qdrant search error: {e}", exc_info=True)
        return []


def generate_response(query: str, context: list, history: list = None) -> tuple:
    """Generate response using OpenAI."""
    client = get_openai_client()
    if not client:
        raise HTTPException(status_code=503, detail="OpenAI service unavailable")

    # Format context
    context_text = "\n\n".join([
        f"[Chapter {c.get('chapter', '?')}: {c.get('section', 'Unknown')}]\n{c.get('content', '')}"
        for c in context
    ])

    # Build messages
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    # Add history if provided
    if history:
        for msg in history[-6:]:  # Last 3 exchanges
            messages.append({"role": msg.role, "content": msg.content})

    # Add current query with context
    user_message = f"""Context from the Physical AI book:
{context_text}

User question: {query}"""

    messages.append({"role": "user", "content": user_message})

    response = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=messages,
        max_tokens=1000,
        temperature=0.3
    )

    answer = response.choices[0].message.content

    # Calculate confidence based on context relevance
    avg_score = sum(c.get("score", 0) for c in context) / len(context) if context else 0
    confidence = min(avg_score, 1.0)

    return answer, confidence


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    RAG Chatbot endpoint - Ask questions about the Physical AI book.

    - Retrieves relevant context from vectorized book content
    - Generates grounded responses with citations
    - Supports conversation history for follow-up questions
    """
    import time
    start_time = time.time()

    logger.info(f"Chat request: {request.message[:50]}...")

    # Check cache
    cache_key = f"{request.message}:{request.language}:{request.chapter}"
    if cache_key in _response_cache:
        cached = _response_cache[cache_key]
        logger.info("Returning cached response")
        return ChatResponse(
            answer=cached["answer"],
            citations=cached["citations"],
            confidence=cached["confidence"],
            response_time_ms=int((time.time() - start_time) * 1000),
            cached=True
        )

    # Retrieve context
    context = retrieve_context(
        request.message,
        language=request.language,
        chapter=request.chapter,
        top_k=5
    )

    if not context:
        # No relevant context found
        return ChatResponse(
            answer="I don't have information about that in the book content. Please ask a question related to Physical AI, ROS 2, Gazebo simulation, or Vision-Language-Action models.",
            citations=[],
            confidence=0.0,
            response_time_ms=int((time.time() - start_time) * 1000),
            cached=False
        )

    # Generate response
    try:
        answer, confidence = generate_response(
            request.message,
            context,
            request.history
        )
    except Exception as e:
        logger.error(f"Response generation error: {e}")
        raise HTTPException(status_code=500, detail="Failed to generate response")

    # Build citations
    citations = []
    seen_sections = set()
    for c in context:
        section_key = f"{c.get('chapter')}:{c.get('section')}"
        if section_key not in seen_sections:
            seen_sections.add(section_key)
            citations.append(Citation(
                chapter=c.get("chapter"),
                section=c.get("section", "Unknown"),
                source=c.get("source", "")
            ))

    response_time_ms = int((time.time() - start_time) * 1000)

    # Cache response
    _response_cache[cache_key] = {
        "answer": answer,
        "citations": citations,
        "confidence": confidence
    }

    logger.info(f"Chat response generated in {response_time_ms}ms, confidence={confidence:.2f}")

    return ChatResponse(
        answer=answer,
        citations=citations,
        confidence=confidence,
        response_time_ms=response_time_ms,
        cached=False
    )


@app.get("/api/chat/health")
async def chat_health():
    """Check RAG chatbot service health."""
    qdrant_ok = get_qdrant_client() is not None
    openai_ok = get_openai_client() is not None

    status = "healthy" if (qdrant_ok and openai_ok) else "degraded"

    return {
        "status": status,
        "qdrant": "connected" if qdrant_ok else "unavailable",
        "openai": "connected" if openai_ok else "unavailable",
        "collection": QDRANT_COLLECTION
    }


# =============================================================================
# Error Handlers
# =============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler for unhandled errors."""
    logger.error(f"Unhandled exception: {type(exc).__name__}: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "error_code": type(exc).__name__},
    )


# =============================================================================
# Vercel Serverless Handler
# =============================================================================

# The app object is automatically detected by @vercel/python
# No additional handler needed for Vercel deployment
