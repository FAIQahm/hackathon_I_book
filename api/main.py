"""
FastAPI Application for Vercel Deployment
Generated by vercel-fastapi-link skill v1.4.0

Project: Physical AI Book API
GitHub Pages: https://physical-ai-book-api.vercel.app
"""

import os
import logging
from datetime import datetime
from typing import Optional, List

from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

# =============================================================================
# Logging Configuration
# =============================================================================
# Configure logging so agents can read Vercel logs for debugging
# Vercel captures stdout/stderr, so we use StreamHandler

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("Physical AI Book API".lower().replace(" ", "-"))

# Adjust log level from environment (DEBUG, INFO, WARNING, ERROR)
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logger.setLevel(getattr(logging, log_level, logging.INFO))

# =============================================================================
# Pydantic Models - Define your request/response schemas here
# =============================================================================

class HealthResponse(BaseModel):
    """Health check response model."""
    status: str = Field(..., example="healthy")
    service: str = Field(..., example="physical-ai-book-api")
    timestamp: str = Field(..., example="2024-01-01T12:00:00Z")


class ChapterSummary(BaseModel):
    """Summary of a chapter for listing."""
    id: int = Field(..., example=1)
    title: str = Field(..., example="Introduction to Physical AI")
    slug: str = Field(..., example="intro")


class ChapterDetail(BaseModel):
    """Full chapter details."""
    id: int
    title: str
    content: str = Field(..., example="Chapter content goes here...")
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


class ChapterListResponse(BaseModel):
    """Response model for chapter listing."""
    chapters: List[ChapterSummary]
    total: int = Field(..., example=3)


class ErrorResponse(BaseModel):
    """Standard error response model."""
    detail: str = Field(..., example="Resource not found")
    error_code: Optional[str] = Field(None, example="NOT_FOUND")


# =============================================================================
# Personalization Models
# =============================================================================

class UserPreferences(BaseModel):
    """User learning preferences."""
    preferred_language: str = Field(default="en", example="en")
    difficulty_level: str = Field(default="beginner", example="intermediate")
    topics_of_interest: List[str] = Field(default=[], example=["robotics", "ai"])
    learning_style: str = Field(default="visual", example="hands-on")
    session_duration_minutes: int = Field(default=30, example=45)


class UserProfile(BaseModel):
    """User profile for personalization."""
    user_id: str = Field(..., example="user-123")
    email: Optional[str] = Field(None, example="user@example.com")
    display_name: Optional[str] = Field(None, example="John Doe")
    preferences: UserPreferences = Field(default_factory=UserPreferences)
    created_at: str = Field(default="", example="2024-01-01T12:00:00Z")
    updated_at: str = Field(default="", example="2024-01-01T12:00:00Z")


class Recommendation(BaseModel):
    """Content recommendation."""
    id: str = Field(..., example="rec-001")
    title: str = Field(..., example="Introduction to ROS 2")
    description: str = Field(..., example="Learn the basics of ROS 2")
    chapter_id: int = Field(..., example=1)
    relevance_score: float = Field(..., ge=0, le=1, example=0.95)
    reason: str = Field(..., example="Based on your interest in robotics")


class RecommendationsResponse(BaseModel):
    """Response with personalized recommendations."""
    user_id: str
    recommendations: List[Recommendation]
    generated_at: str


class LearningPathItem(BaseModel):
    """Single item in learning path."""
    order: int = Field(..., example=1)
    chapter_id: int = Field(..., example=1)
    title: str = Field(..., example="Getting Started")
    status: str = Field(default="not_started", example="in_progress")
    progress_percent: int = Field(default=0, ge=0, le=100, example=50)
    estimated_duration_minutes: int = Field(default=30, example=45)


class LearningPath(BaseModel):
    """User's personalized learning path."""
    user_id: str
    path_id: str = Field(..., example="path-001")
    title: str = Field(default="My Learning Journey", example="Physical AI Mastery")
    items: List[LearningPathItem]
    overall_progress_percent: int = Field(default=0, ge=0, le=100)
    created_at: str
    updated_at: str


class PersonalizationSettings(BaseModel):
    """Settings to apply for personalization."""
    user_id: str = Field(..., example="user-123")
    preferences: UserPreferences
    notifications_enabled: bool = Field(default=True)
    theme: str = Field(default="auto", example="dark")


class PersonalizationApplyResponse(BaseModel):
    """Response after applying personalization settings."""
    success: bool
    message: str
    applied_at: str


# =============================================================================
# Chat/RAG Models
# =============================================================================

class ChatMessage(BaseModel):
    """A single chat message."""
    role: str = Field(..., example="user")
    content: str = Field(..., example="What is Physical AI?")


class ChatRequest(BaseModel):
    """Request model for chat endpoint."""
    message: str = Field(..., min_length=1, max_length=500, example="What is Physical AI?")
    language: str = Field(default="en", example="en")
    history: List[ChatMessage] = Field(default=[], description="Previous conversation messages")


class Citation(BaseModel):
    """A citation referencing source material."""
    chapter: Optional[int] = Field(None, example=1)
    section: str = Field(..., example="Introduction")
    source: str = Field(default="", example="docs/chapter-1/index.md")


class ChatResponse(BaseModel):
    """Response model for chat endpoint."""
    answer: str = Field(..., example="Physical AI refers to...")
    citations: List[Citation] = Field(default=[])
    confidence: float = Field(default=0.0, ge=0, le=1, example=0.85)


# =============================================================================
# App Configuration
# =============================================================================

app = FastAPI(
    title="Physical AI Book API",
    description="Backend API for the Physical AI Educational Book",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

logger.info("FastAPI application initialized")

# =============================================================================
# CORS Configuration - Allow GitHub Pages frontend
# =============================================================================

# Get GitHub Pages URL from environment or use default
GITHUB_PAGES_URL = os.getenv("GITHUB_PAGES_URL", "https://physical-ai-book-api.vercel.app")

# Base allowed origins
allowed_origins = [
    "http://localhost:3000",      # Local Docusaurus dev
    "http://localhost:8000",      # Local FastAPI dev
    "http://127.0.0.1:3000",
    "http://127.0.0.1:8000",
    GITHUB_PAGES_URL,             # Production GitHub Pages
    f"{GITHUB_PAGES_URL}/",       # With trailing slash
]

# Add extra origins from environment variable (comma-separated)
extra_origins_env = os.getenv("EXTRA_CORS_ORIGINS", "")
if extra_origins_env:
    for origin in extra_origins_env.split(","):
        origin = origin.strip()
        if origin and origin not in allowed_origins:
            allowed_origins.append(origin)

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
    allow_headers=["Authorization", "Content-Type", "Accept", "Origin"],
    max_age=600,
)

# =============================================================================
# Health Check Endpoint
# =============================================================================

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for monitoring."""
    logger.debug("Health check requested")
    return HealthResponse(
        status="healthy",
        service="physical-ai-book-api",
        timestamp=datetime.utcnow().isoformat() + "Z",
    )


@app.get("/")
async def root():
    """Root endpoint with API info."""
    logger.info("Root endpoint accessed")
    return {
        "message": "Physical AI Book API",
        "docs": "/docs",
        "health": "/health",
    }

# =============================================================================
# API Routes - Add your endpoints below
# =============================================================================

@app.get("/api/v1/chapters", response_model=ChapterListResponse)
async def list_chapters():
    """List all available chapters."""
    logger.info("Listing all chapters")
    chapters = [
        ChapterSummary(id=1, title="Introduction to Physical AI", slug="intro"),
        ChapterSummary(id=2, title="Sensors and Perception", slug="chapter-1"),
        ChapterSummary(id=3, title="Actuators and Control", slug="chapter-2"),
    ]
    logger.debug(f"Returning {len(chapters)} chapters")
    return ChapterListResponse(chapters=chapters, total=len(chapters))


@app.get(
    "/api/v1/chapters/{chapter_id}",
    response_model=ChapterDetail,
    responses={404: {"model": ErrorResponse}},
)
async def get_chapter(chapter_id: int):
    """Get a specific chapter by ID."""
    logger.info(f"Fetching chapter with id={chapter_id}")

    chapters = {
        1: ChapterDetail(id=1, title="Introduction to Physical AI", content="..."),
        2: ChapterDetail(id=2, title="Sensors and Perception", content="..."),
        3: ChapterDetail(id=3, title="Actuators and Control", content="..."),
    }

    if chapter_id not in chapters:
        logger.warning(f"Chapter not found: id={chapter_id}")
        raise HTTPException(status_code=404, detail="Chapter not found")

    logger.debug(f"Returning chapter: {chapters[chapter_id].title}")
    return chapters[chapter_id]


# =============================================================================
# Personalization Endpoints
# =============================================================================

# In-memory storage for demo (replace with database in production)
_user_profiles: dict = {}
_user_learning_paths: dict = {}

# Onboarding questions based on 10-dimension user profile
ONBOARDING_QUESTIONS = [
    {
        "dimension": "learning_style",
        "question": "How do you prefer to learn new concepts?",
        "options": [
            {"value": "visual", "label": "Visual - Diagrams, charts, and images"},
            {"value": "reading", "label": "Reading - Text and written explanations"},
            {"value": "kinesthetic", "label": "Hands-on - Interactive exercises and practice"},
            {"value": "auditory", "label": "Auditory - Verbal explanations and discussions"}
        ]
    },
    {
        "dimension": "knowledge_level",
        "question": "What is your current experience with robotics and AI?",
        "options": [
            {"value": "beginner", "label": "Beginner - New to these topics"},
            {"value": "intermediate", "label": "Intermediate - Some experience"},
            {"value": "advanced", "label": "Advanced - Experienced practitioner"}
        ]
    },
    {
        "dimension": "content_depth",
        "question": "How detailed would you like the explanations to be?",
        "options": [
            {"value": "overview", "label": "Overview - Quick summaries and key points"},
            {"value": "standard", "label": "Standard - Balanced detail with examples"},
            {"value": "deep-dive", "label": "Deep-dive - Comprehensive technical details"}
        ]
    },
    {
        "dimension": "example_preference",
        "question": "What type of examples help you learn best?",
        "options": [
            {"value": "theoretical", "label": "Theoretical - Concepts and principles"},
            {"value": "practical", "label": "Practical - Real-world applications"},
            {"value": "code-heavy", "label": "Code-heavy - Code snippets and implementations"}
        ]
    },
    {
        "dimension": "goal_orientation",
        "question": "What is your primary learning goal?",
        "options": [
            {"value": "understanding", "label": "Understanding - Deep conceptual knowledge"},
            {"value": "application", "label": "Application - Build real projects"},
            {"value": "certification", "label": "Certification - Formal credentials"}
        ]
    },
    {
        "dimension": "language",
        "question": "Which language do you prefer for content?",
        "options": [
            {"value": "en", "label": "English"},
            {"value": "ur", "label": "اردو (Urdu)"}
        ]
    }
]


@app.get("/api/personalization/questions")
async def get_onboarding_questions():
    """Get personalization onboarding questions."""
    logger.info("Fetching onboarding questions")
    return ONBOARDING_QUESTIONS


@app.get(
    "/api/personalization/profile",
    response_model=UserProfile,
    responses={404: {"model": ErrorResponse}},
)
async def get_user_profile(user_id: str):
    """Get user profile for personalization."""
    logger.info(f"Fetching profile for user_id={user_id}")

    if user_id in _user_profiles:
        logger.debug(f"Found existing profile for {user_id}")
        return _user_profiles[user_id]

    # Return default profile for new users
    logger.info(f"Creating default profile for new user {user_id}")
    default_profile = UserProfile(
        user_id=user_id,
        preferences=UserPreferences(),
        created_at=datetime.utcnow().isoformat() + "Z",
        updated_at=datetime.utcnow().isoformat() + "Z",
    )
    _user_profiles[user_id] = default_profile
    return default_profile


@app.post("/api/personalization/profile", response_model=UserProfile)
async def update_user_profile(profile: UserProfile):
    """Create or update user profile."""
    logger.info(f"Updating profile for user_id={profile.user_id}")

    profile.updated_at = datetime.utcnow().isoformat() + "Z"
    if profile.user_id not in _user_profiles:
        profile.created_at = profile.updated_at

    _user_profiles[profile.user_id] = profile
    logger.debug(f"Profile saved for {profile.user_id}")
    return profile


@app.get("/api/personalization/recommendations", response_model=RecommendationsResponse)
async def get_recommendations(user_id: str):
    """Get AI-driven content recommendations for user."""
    logger.info(f"Generating recommendations for user_id={user_id}")

    # Get user preferences for personalization
    profile = _user_profiles.get(user_id)
    topics = profile.preferences.topics_of_interest if profile else []

    # Generate recommendations based on preferences (demo logic)
    recommendations = [
        Recommendation(
            id="rec-001",
            title="Introduction to Physical AI",
            description="Start your journey into Physical AI and robotics",
            chapter_id=1,
            relevance_score=0.95,
            reason="Recommended starting point for all learners",
        ),
        Recommendation(
            id="rec-002",
            title="ROS 2 Fundamentals",
            description="Learn the Robot Operating System",
            chapter_id=1,
            relevance_score=0.88,
            reason="Essential foundation for robotics" + (f" - matches your interest in {topics[0]}" if topics else ""),
        ),
        Recommendation(
            id="rec-003",
            title="Simulation with Gazebo",
            description="Practice in a virtual environment",
            chapter_id=2,
            relevance_score=0.82,
            reason="Hands-on learning without hardware",
        ),
    ]

    logger.debug(f"Generated {len(recommendations)} recommendations for {user_id}")
    return RecommendationsResponse(
        user_id=user_id,
        recommendations=recommendations,
        generated_at=datetime.utcnow().isoformat() + "Z",
    )


@app.get("/api/personalization/learning-path", response_model=LearningPath)
async def get_learning_path(user_id: str):
    """Get personalized learning path for user."""
    logger.info(f"Fetching learning path for user_id={user_id}")

    if user_id in _user_learning_paths:
        logger.debug(f"Found existing learning path for {user_id}")
        return _user_learning_paths[user_id]

    # Generate default learning path
    logger.info(f"Creating default learning path for {user_id}")
    now = datetime.utcnow().isoformat() + "Z"

    learning_path = LearningPath(
        user_id=user_id,
        path_id=f"path-{user_id}",
        title="Physical AI Learning Journey",
        items=[
            LearningPathItem(
                order=1,
                chapter_id=1,
                title="Introduction to Physical AI & ROS 2",
                status="not_started",
                progress_percent=0,
                estimated_duration_minutes=60,
            ),
            LearningPathItem(
                order=2,
                chapter_id=2,
                title="Simulation with Gazebo",
                status="not_started",
                progress_percent=0,
                estimated_duration_minutes=90,
            ),
            LearningPathItem(
                order=3,
                chapter_id=3,
                title="Vision-Language-Action Models",
                status="not_started",
                progress_percent=0,
                estimated_duration_minutes=120,
            ),
        ],
        overall_progress_percent=0,
        created_at=now,
        updated_at=now,
    )

    _user_learning_paths[user_id] = learning_path
    return learning_path


@app.post("/api/personalization/apply", response_model=PersonalizationApplyResponse)
async def apply_personalization(settings: PersonalizationSettings):
    """Apply personalization settings for user."""
    logger.info(f"Applying personalization for user_id={settings.user_id}")

    # Update or create profile with new preferences
    profile = _user_profiles.get(settings.user_id)
    now = datetime.utcnow().isoformat() + "Z"

    if profile:
        profile.preferences = settings.preferences
        profile.updated_at = now
    else:
        profile = UserProfile(
            user_id=settings.user_id,
            preferences=settings.preferences,
            created_at=now,
            updated_at=now,
        )

    _user_profiles[settings.user_id] = profile

    logger.info(f"Personalization applied for {settings.user_id}: theme={settings.theme}, notifications={settings.notifications_enabled}")

    return PersonalizationApplyResponse(
        success=True,
        message=f"Personalization settings applied successfully for user {settings.user_id}",
        applied_at=now,
    )


# =============================================================================
# Chat/RAG Endpoint
# =============================================================================

# Simple response cache for demo
_chat_cache: dict = {}


def _get_rag_context(query: str, language: str = "en") -> tuple[list[dict], list[Citation]]:
    """
    Retrieve relevant context from Qdrant for RAG.
    Returns (context_chunks, citations).
    """
    try:
        from qdrant_client import QdrantClient
        from qdrant_client.models import Filter, FieldCondition, MatchValue
        from openai import OpenAI as OpenAIClient

        qdrant_url = os.getenv("QDRANT_URL")
        qdrant_api_key = os.getenv("QDRANT_API_KEY")
        openai_api_key = os.getenv("OPENAI_API_KEY")

        if not all([qdrant_url, qdrant_api_key, openai_api_key]):
            logger.warning("RAG credentials not configured, using fallback")
            return [], []

        # Create embedding for query
        openai_client = OpenAIClient(api_key=openai_api_key)
        embed_response = openai_client.embeddings.create(
            input=query[:8000],
            model="text-embedding-3-small"
        )
        query_vector = embed_response.data[0].embedding

        # Query Qdrant
        qdrant = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)

        # Build filter for language
        query_filter = Filter(
            must=[FieldCondition(key="language", match=MatchValue(value=language))]
        )

        results = qdrant.search(
            collection_name="textbook_chapters",
            query_vector=query_vector,
            query_filter=query_filter,
            limit=5
        )

        context_chunks = []
        citations = []
        seen_sources = set()

        for hit in results:
            if hit.score < 0.5:  # Skip low relevance results
                continue

            payload = hit.payload or {}
            context_chunks.append({
                "content": payload.get("content", ""),
                "section": payload.get("section", ""),
                "chapter": payload.get("chapter"),
                "score": hit.score
            })

            # Add unique citations
            source = payload.get("source", "")
            if source not in seen_sources:
                seen_sources.add(source)
                citations.append(Citation(
                    chapter=payload.get("chapter"),
                    section=payload.get("section", "Unknown"),
                    source=source
                ))

        return context_chunks, citations

    except Exception as e:
        logger.error(f"RAG retrieval error: {e}")
        return [], []


def _generate_answer(query: str, context: list[dict], history: list[ChatMessage]) -> tuple[str, float]:
    """
    Generate an answer using OpenAI GPT based on retrieved context.
    Returns (answer, confidence).
    """
    openai_api_key = os.getenv("OPENAI_API_KEY")

    if not openai_api_key:
        logger.warning("OpenAI API key not configured")
        return _get_fallback_answer(query), 0.3

    try:
        from openai import OpenAI as OpenAIClient

        client = OpenAIClient(api_key=openai_api_key)

        # Build context string
        if context:
            context_text = "\n\n".join([
                f"[Chapter {c.get('chapter', '?')}: {c.get('section', 'Unknown')}]\n{c.get('content', '')}"
                for c in context
            ])
            system_prompt = f"""You are a helpful assistant for the Physical AI educational textbook.
Answer questions based ONLY on the provided context from the book.
If the question cannot be answered from the context, say "I don't have information about that in the Physical AI book content."
Be concise but informative.

Context from the book:
{context_text}"""
        else:
            system_prompt = """You are a helpful assistant for the Physical AI educational textbook.
You don't have specific content loaded right now. Provide general guidance about Physical AI topics like ROS 2, robotics, Gazebo simulation, and AI.
If asked about specific book content, suggest the user check the relevant chapter."""

        # Build messages with history
        messages = [{"role": "system", "content": system_prompt}]

        # Add recent conversation history
        for msg in history[-4:]:  # Last 4 messages for context
            messages.append({"role": msg.role, "content": msg.content})

        messages.append({"role": "user", "content": query})

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=500,
            temperature=0.7
        )

        answer = response.choices[0].message.content

        # Calculate confidence based on context relevance
        confidence = 0.85 if context else 0.5
        if context:
            avg_score = sum(c.get("score", 0) for c in context) / len(context)
            confidence = min(0.95, avg_score)

        return answer, confidence

    except Exception as e:
        logger.error(f"OpenAI generation error: {e}")
        return _get_fallback_answer(query), 0.3


def _get_fallback_answer(query: str) -> str:
    """Provide a fallback answer when RAG is unavailable."""
    query_lower = query.lower()

    # Simple keyword-based fallbacks
    if "physical ai" in query_lower:
        return "Physical AI refers to AI systems that interact with the physical world through robots and sensors. It combines machine learning, computer vision, and robotics to create intelligent physical systems."
    elif "ros" in query_lower or "robot operating system" in query_lower:
        return "ROS 2 (Robot Operating System 2) is an open-source robotics middleware framework. It provides tools and libraries for building robot applications, including communication, hardware abstraction, and simulation support."
    elif "gazebo" in query_lower:
        return "Gazebo is a powerful 3D robotics simulator that integrates with ROS 2. It allows you to test robot designs in realistic virtual environments before deploying to real hardware."
    elif "install" in query_lower:
        return "For installation instructions, please refer to the specific chapter in the Physical AI book. ROS 2 installation typically involves adding the ROS repository and installing packages via apt (Ubuntu) or other package managers."
    else:
        return "I'm having trouble connecting to the knowledge base right now. Please try again in a moment, or browse the chapters directly for information on Physical AI, ROS 2, and robotics."


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    RAG-powered chat endpoint for the Physical AI chatbot.

    Retrieves relevant content from the book and generates an answer.
    """
    logger.info(f"Chat request: '{request.message[:50]}...' (lang={request.language})")

    # Check cache first
    cache_key = f"{request.message}:{request.language}"
    if cache_key in _chat_cache:
        logger.debug("Returning cached response")
        return _chat_cache[cache_key]

    # Get RAG context
    context, citations = _get_rag_context(request.message, request.language)
    logger.debug(f"Retrieved {len(context)} context chunks, {len(citations)} citations")

    # Generate answer
    answer, confidence = _generate_answer(request.message, context, request.history)

    response = ChatResponse(
        answer=answer,
        citations=citations,
        confidence=confidence
    )

    # Cache successful responses
    if confidence > 0.5:
        _chat_cache[cache_key] = response

    logger.info(f"Chat response generated (confidence={confidence:.2f})")
    return response


# =============================================================================
# Error Handlers
# =============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler for unhandled errors."""
    logger.error(f"Unhandled exception: {type(exc).__name__}: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "error_code": type(exc).__name__},
    )


# =============================================================================
# Vercel Serverless Handler
# =============================================================================

# The app object is automatically detected by @vercel/python
# No additional handler needed for Vercel deployment
